{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bosy-Ayman/DSAI-403-Nature-Inspired-Computation/blob/main/Lab2_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9hINxHm0vEV"
      },
      "source": [
        "# Lab 2: Hyperparameter Tuning with Hill Climbing (NLP Project)\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this lab, you will be able to:\n",
        "1. Understand how **Hill Climbing** search can be applied for hyperparameter tuning in NLP models.  \n",
        "2. Explore the trade-offs between **greedy local optimization** and **global exploration**.  \n",
        "3. Implement, evaluate, and analyze a **classification model on text data**.  \n",
        "4. Compare **basic Hill Climbing** with **improved Hill Climbing (First-Ascent with Restarts)**.  \n",
        "\n",
        "---\n",
        "\n",
        "## Task Overview\n",
        "In this lab, you will work with a **text classification dataset** (e.g., IMDB reviews, News classification, or SMS spam dataset).  \n",
        "Your goal is to tune the hyperparameters of a machine learning model using **Hill Climbing**.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset\n",
        "- Choose an **NLP dataset** (sentiment analysis or text classification). Examples:  \n",
        "  - IMDB Movie Reviews (binary sentiment)  \n",
        "  - SMS Spam Collection (spam vs. ham)  \n",
        "  - News Category Dataset  \n",
        "\n",
        "---\n",
        "\n",
        "## Lab Tasks\n",
        "\n",
        "### Step 1: Data Preparation\n",
        "- Load the dataset.  \n",
        "- Perform preprocessing:  \n",
        "  - Tokenization  \n",
        "  - Stopword removal  \n",
        "  - TF-IDF or CountVectorizer transformation  \n",
        "- Split the dataset into **training** and **validation** sets.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 2: Define the Model\n",
        "- Select a baseline ML model (e.g., **Logistic Regression**, **Naive Bayes**, or **SVM**).  \n",
        "- Identify hyperparameters to tune. Examples:  \n",
        "  - Learning rate  \n",
        "  - Regularization strength (`C` for Logistic Regression, `alpha` for Naive Bayes, etc.)  \n",
        "  - Maximum features for TF-IDF  \n",
        "  - n-gram range  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 3: Define Search Space\n",
        "- Clearly specify a **discrete search space** for each hyperparameter. Example:  \n",
        "  - Learning rate: [0.001, 0.01, 0.1, 1]  \n",
        "  - Regularization (C): [0.1, 1, 10]  \n",
        "  - Max features: [1000, 5000, 10000]  \n",
        "  - N-gram range: [(1,1), (1,2), (1,3)]  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 4: Implement Hill Climbing\n",
        "- Start with **randomly selected hyperparameters**.  \n",
        "- Generate **neighbors** by changing one hyperparameter at a time.  \n",
        "- Evaluate model performance (validation accuracy) for each neighbor.  \n",
        "- Move to the neighbor if it improves performance.  \n",
        "- Stop if no improvement is found.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 5: Improved Hill Climbing (First-Ascent with Restarts)\n",
        "- Implement **First-Ascent strategy**: accept the first better neighbor you find.  \n",
        "- Add **random restarts** to escape local optima.  \n",
        "- Keep track of the **best overall solution** across restarts.  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 6: Model Training and Evaluation\n",
        "- Compile and train the **final best model** with tuned hyperparameters.  \n",
        "- Evaluate on the **test set**.  \n",
        "- Report:  \n",
        "  - Best hyperparameters found  \n",
        "  - Training vs. validation accuracy  \n",
        "  - Test accuracy  \n",
        "\n",
        "---\n",
        "\n",
        "### Step 7: Visualization\n",
        "- Plot the **accuracy progression over iterations** of Hill Climbing.  \n",
        "- Show how hyperparameters evolved during search.  \n",
        "- Compare **baseline vs. tuned model** performance.  \n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "At the end of this lab, you must submit:\n",
        "1. Preprocessed dataset description (with shapes and samples).  \n",
        "2. Implementation of **basic and improved Hill Climbing**.  \n",
        "3. Visualization of the tuning process (accuracy vs. iterations).  \n",
        "4. Final report with:  \n",
        "   - Best hyperparameters found  \n",
        "   - Validation and test accuracy  \n",
        "   - Observations on differences between basic and improved Hill Climbing  \n",
        "\n",
        "---\n",
        "\n",
        "## Reflection Questions\n",
        "- Why does Hill Climbing sometimes get stuck in local optima?  \n",
        "- How do **restarts** help in escaping local optima?  \n",
        "- Compare **Hill Climbing** with **Grid Search** and **Random Search** in terms of efficiency and results.  \n",
        "- What would happen if the search space is **continuous** instead of discrete?  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KgSAfL3M9Y5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation, Rescaling\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Q-tLDLfNKSN"
      },
      "outputs": [],
      "source": [
        "seed = 123\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set memory growth for each GPU\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        # Use only the first GPU\n",
        "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "        print('\\nGPU Found! Using GPU...')\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy()\n",
        "    print('Number of replicas:', strategy.num_replicas_in_sync)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBKNtJNLNOUj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# =======================================================\n",
        "# Step 1: Load and Prepare CIFAR-10 Dataset\n",
        "# =======================================================\n",
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "# Use 10% of the training data as validation set\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.1, random_state=seed, stratify=y_train_full\n",
        ")\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "y_train_cat = to_categorical(y_train, num_classes=10)\n",
        "y_val_cat = to_categorical(y_val, num_classes=10)\n",
        "y_test_cat = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Rescale pixel values (0-255 to 0-1)\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_val = X_val.astype('float32') / 255.0\n",
        "X_test = X_test.astype('float32') / 255.0\n",
        "\n",
        "# Prepare TensorFlow Datasets (for Keras compatibility, similar to original code structure)\n",
        "# Note: CIFAR-10 images are 32x32x3\n",
        "BATCH_SIZE = 32 # Temporary batch size, the optimal one will be tuned later\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat)).shuffle(10000).batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat)).batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test_cat)).batch(BATCH_SIZE).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# --- Data Augmentation Pipeline (Adapted for CIFAR-10 32x32 size) ---\n",
        "augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "        tf.keras.layers.RandomRotation(\n",
        "        factor = (-.1, .1), # Smaller rotation for smaller images\n",
        "        fill_mode = 'reflect',\n",
        "        interpolation = 'bilinear',\n",
        "        seed = seed),\n",
        "        tf.keras.layers.RandomContrast(\n",
        "        factor = (.2),\n",
        "        seed = seed)\n",
        "    ], name=\"data_augmentation\"\n",
        ")\n",
        "augmentation.build((None, 32, 32, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFZDXDaaNP9f"
      },
      "outputs": [],
      "source": [
        "search_space = {\n",
        "    \"learning_rate\": [1e-4, 1e-3, 5e-3],\n",
        "    \"dropout_rate\": [0.3, 0.5],\n",
        "    \"filters\": [32, 64],\n",
        "    \"batch_size\": [32, 64] # Updated options\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GadtL5IPNW0T"
      },
      "outputs": [],
      "source": [
        "def build_model(params):\n",
        "    with strategy.scope():\n",
        "        model = Sequential()\n",
        "        model.add(tf.keras.Input(shape=(32, 32, 3))) # Explicit input shape for 32x32x3\n",
        "        model.add(augmentation)\n",
        "\n",
        "        # First Conv Block\n",
        "        model.add(Conv2D(params[\"filters\"], (3,3), padding=\"same\"))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), padding=\"same\"))\n",
        "        model.add(Dropout(params[\"dropout_rate\"]))\n",
        "\n",
        "        # Second Conv Block (fixed 64 filters)\n",
        "        model.add(Conv2D(64, (3,3), padding=\"same\"))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), padding=\"same\"))\n",
        "        model.add(Dropout(params[\"dropout_rate\"]))\n",
        "\n",
        "        # Third Conv Block (fixed 128 filters)\n",
        "        model.add(Conv2D(128, (3,3), padding=\"same\"))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(MaxPooling2D(pool_size=(2,2), padding=\"same\"))\n",
        "        model.add(Dropout(params[\"dropout_rate\"]))\n",
        "\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation=\"relu\"))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(10, activation=\"softmax\")) # 10 classes for CIFAR-10\n",
        "\n",
        "        optimizer = tf.keras.optimizers.RMSprop(params[\"learning_rate\"])\n",
        "        model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vToxQKqyNZQx"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  For hyperparameter tuning, we use 'val_ds' which is X_val/y_val_cat\n",
        "def evaluate_model(params):\n",
        "    model = build_model(params)\n",
        "    # Dynamically change dataset batch size for training based on params\n",
        "    train_ds_tuned = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat)).shuffle(10000).batch(params[\"batch_size\"]).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    val_ds_tuned = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat)).batch(params[\"batch_size\"]).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_ds_tuned,\n",
        "        validation_data=val_ds_tuned,\n",
        "        epochs=3,  # train only a few epochs for speed\n",
        "        verbose=0\n",
        "    )\n",
        "    val_acc = max(history.history.get(\"val_accuracy\", [0]))\n",
        "    # Free up memory\n",
        "    del model\n",
        "    tf.keras.backend.clear_session()\n",
        "    return val_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgU5r-doNvva"
      },
      "outputs": [],
      "source": [
        "\n",
        "def hill_climb(max_iter=5):\n",
        "    current = {k: random.choice(v) for k, v in search_space.items()}\n",
        "    current_score = evaluate_model(current)\n",
        "    print(f\"Initial Params: {current} | Accuracy: {current_score:.4f}\")\n",
        "\n",
        "    history = [(0, current_score)]\n",
        "\n",
        "    for i in range(1, max_iter + 1):\n",
        "        # Generate neighbor by changing exactly one random parameter\n",
        "        neighbor = copy.deepcopy(current)\n",
        "        key = random.choice(list(search_space.keys()))\n",
        "\n",
        "        # Ensure the new value is actually different from the current one\n",
        "        available_values = [v for v in search_space[key] if v != current[key]]\n",
        "        if not available_values: # If there's only one choice, skip this iteration\n",
        "             history.append((i, current_score))\n",
        "             continue\n",
        "\n",
        "        neighbor[key] = random.choice(available_values)\n",
        "        score = evaluate_model(neighbor)\n",
        "\n",
        "        print(f\"Iteration {i}: Tried {neighbor} -> Acc={score:.4f}\")\n",
        "\n",
        "        if score > current_score:\n",
        "            print(\"  Improvement found! Updating current solution.\")\n",
        "            current, current_score = neighbor, score\n",
        "\n",
        "        history.append((i, current_score))\n",
        "\n",
        "    return current, current_score, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h3UFIgoN0OD"
      },
      "outputs": [],
      "source": [
        "\n",
        "def hill_climb_with_restarts(max_iter=5, restarts=3):\n",
        "    best_overall = None\n",
        "    best_score = 0\n",
        "    all_history = []\n",
        "\n",
        "    for r in range(restarts):\n",
        "        print(f\"\\n=== Restart {r+1} ===\")\n",
        "        current, score, history = hill_climb(max_iter)\n",
        "        all_history.extend([(r*max_iter + i, acc) for i, acc in history])\n",
        "\n",
        "        if score > best_score:\n",
        "            best_overall, best_score = current, score\n",
        "\n",
        "    return best_overall, best_score, all_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1oOL22u9N1-s"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Starting Hill Climbing Hyperparameter Search for CIFAR-10 CNN\")\n",
        "print(\"=\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "eczuLfJzMxIm",
        "outputId": "8de9891a-d26e-49a5-80cd-4640d576e7a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of replicas: 1\n",
            "\n",
            "==================================================\n",
            "Starting Hill Climbing Hyperparameter Search for CIFAR-10 CNN\n",
            "==================================================\n",
            "\n",
            "=== Restart 1 ===\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Reduced max_iter and restarts for quicker example run; adjust if necessary\n",
        "best_params, best_score, history = hill_climb_with_restarts(max_iter=4, restarts=2)\n",
        "# Note: A real search would use more iterations/restarts\n",
        "\n",
        "print(\"\\nBest Hyperparameters Found:\")\n",
        "print(best_params)\n",
        "print(f\"Best Validation Accuracy (after {3} epochs): {best_score:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrWOA8GIOMq_"
      },
      "outputs": [],
      "source": [
        "if history:\n",
        "    iterations, accuracies = zip(*history)\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.plot(iterations, accuracies, marker='o')\n",
        "    plt.title(\"Hill Climbing Accuracy Progression (CIFAR-10)\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    plt.ylabel(\"Validation Accuracy\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSUx3ZSnOHv_"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training Final Model with Best Parameters\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define Early Stopping and Model Checkpoints (unchanged)\n",
        "early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
        "                              patience = 5, mode = 'max',\n",
        "                              restore_best_weights = True)\n",
        "\n",
        "checkpoint = ModelCheckpoint('best_cifar10_model.h5',\n",
        "                            monitor = 'val_accuracy',\n",
        "                            save_best_only = True)\n",
        "\n",
        "final_model = build_model(best_params)\n",
        "\n",
        "# Retrain with the optimal batch size and the full training dataset\n",
        "final_train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train_cat)).shuffle(10000).batch(best_params[\"batch_size\"]).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "final_val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val_cat)).batch(best_params[\"batch_size\"]).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Use the X_val/y_val for monitoring (Test set used only for final evaluation)\n",
        "history = final_model.fit(\n",
        "    final_train_ds,\n",
        "    validation_data=final_val_ds,\n",
        "    epochs=50, # Set a high number of epochs, EarlyStopping will stop it\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WWa2ol5OEGY"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Plotting Training History\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Extract metrics from training history\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "epochs_ran = range(1, len(loss) + 1)\n",
        "\n",
        "# Create subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# ---- Loss Plot ----\n",
        "ax1.plot(epochs_ran, loss, 'b-', linewidth=2.5, label='Training Loss')\n",
        "ax1.plot(epochs_ran, val_loss, 'r-', linewidth=2.5, label='Validation Loss')\n",
        "ax1.set_title('Loss Over Epochs', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "ax1.legend()\n",
        "\n",
        "# ---- Accuracy Plot ----\n",
        "ax2.plot(epochs_ran, acc, 'b-', linewidth=2.5, label='Training Accuracy')\n",
        "ax2.plot(epochs_ran, val_acc, 'r-', linewidth=2.5, label='Validation Accuracy')\n",
        "ax2.set_title('Accuracy Over Epochs', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "ax2.legend()\n",
        "\n",
        "# Adjust layout\n",
        "plt.suptitle('Loss and Accuracy Over Epochs (CIFAR-10)', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_5wIFcCN9mc"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Final Evaluation on Test Set\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Load best weights before final evaluation\n",
        "final_model.load_weights('best_cifar10_model.h5')\n",
        "\n",
        "test_loss, test_acc = final_model.evaluate(test_ds)\n",
        "\n",
        "print('\\nTest Loss: ', test_loss)\n",
        "print(f'Test Accuracy: {np.round(test_acc * 100, 2)}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaEpj7S2N6u0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Select a random sample from the test set for prediction\n",
        "sample_index = random.randint(0, len(X_test) - 1)\n",
        "sample_image = X_test[sample_index]\n",
        "true_label = class_names[np.argmax(y_test_cat[sample_index])]\n",
        "\n",
        "# Model prediction requires batch dimension\n",
        "preds = final_model.predict(np.expand_dims(sample_image, axis=0))\n",
        "preds_class = np.argmax(preds)\n",
        "preds_label = class_names[preds_class]\n",
        "confidence_score = preds[0][preds_class]\n",
        "\n",
        "print(f'\\n--- Prediction Example ---')\n",
        "print(f'True Class: {true_label}')\n",
        "print(f'Predicted Class: {preds_label}')\n",
        "print(f'Confidence Score: {confidence_score:.4f}')\n",
        "\n",
        "# Display the image\n",
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(sample_image)\n",
        "plt.title(f\"True: {true_label} | Predicted: {preds_label} ({confidence_score:.2%})\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}